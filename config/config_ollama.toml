# ParManus AI Configuration - Ollama Setup
[llm]
# Use Ollama as primary (since no local models found)
api_type = "ollama"
model = "llama3.2"
base_url = "http://localhost:11434/v1"
api_key = "ollama"
max_tokens = 2048
temperature = 0.0

# Local model settings (for when you get GGUF models)
model_path = "models/Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf"
n_gpu_layers = -1
gpu_memory_limit = 7000

# Vision model configuration
[llm.vision]
enabled = true
api_type = "ollama"
model = "llama3.2-vision"
base_url = "http://localhost:11434/v1"
api_key = "ollama"
max_tokens = 2048
temperature = 0.0

# Local vision model settings (for when you get GGUF models)
# model_path = "models/llava-1.6-mistral-7b-gguf/ggml-model-q4_k.gguf"
# clip_model_path = "models/llava-1.6-mistral-7b-gguf/mmproj-model-f16.gguf"

# Browser configuration
[browser]
headless = false
disable_security = true
extra_chromium_args = []

# Memory configuration
[memory]
save_session = false
recover_last_session = false
memory_compression = false

# Voice configuration (optional)
[voice]
speak = false
listen = false
agent_name = "Friday"
