# ParManus AI Configuration - Local GGUF Models
[llm]
# Use local GGUF models (primary)
api_type = "local"
model = "Llama-3.2-11B-Vision-Instruct"
model_path = "models/Llama-3.2-11B-Vision-Instruct.Q4_K_M.gguf"
max_tokens = 2048
temperature = 0.0
n_gpu_layers = -1                   # Use all available GPU layers
gpu_memory_limit = 7000             # Optimized for 8GB VRAM (RTX 3070)

# Ollama fallback configuration (if local model fails)
base_url = "http://localhost:11434/v1"
api_key = "ollama"

# Vision model configuration
[llm.vision]
enabled = true
model = "llava-v1.6-mistral-7b"
model_path = "models/llava-1.6-mistral-7b-gguf/ggml-model-q4_k.gguf"
clip_model_path = "models/llava-1.6-mistral-7b-gguf/mmproj-model-f16.gguf"
max_tokens = 2048
temperature = 0.0
n_gpu_layers = -1

# Browser configuration
[browser]
headless = false
disable_security = true
extra_chromium_args = []

# Memory configuration
[memory]
save_session = false
recover_last_session = false
memory_compression = false

# Voice configuration (optional)
[voice]
speak = false
listen = false
agent_name = "Friday"

